---
title: Configuration
weight: 3
---

This page captures some of the most important options for configuring the custom resources for monitoring.

For information on configuring custom Prometheus metrics and alerting rules, refer to the upstream documentation for the [Prometheus operator.](https://github.com/prometheus-operator/prometheus-operator) Some of the most important custom resources are explained in the Prometheus [design documentation.](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/design.md) The Prometheus documentation can help also you set up RBAC, Thanos, or custom configuration. 

- [About Prometheus Configuration](#about-prometheus-configuration)
- [Configuring Targets with ServiceMonitors and PodMonitors](#configuring-targets-with-servicemonitors-and-podmonitors)
  - [ServiceMonitors](#servicemonitors)
  - [PodMonitors](#podmonitors)
- [Configuring Alerts or Recording Rules with PrometheusRules](#configuring-alerts-or-recording-rules-with-prometheusrules)
  - [PrometheusRules](#prometheusrules)
  - [Recording Rules](#recording-rules)
  - [Alertmanager Config](#alertmanager-config)
- [Trusted CA for Notifiers](#trusted-ca-for-notifiers)
- [Additional Scrape Configurations](#additional-scrape-configurations)
- [Examples](#examples)

# About Prometheus Configuration

The primary way that users will be able to customize this feature for specific Monitoring and Alerting use cases is by creating and/or modifying ConfigMaps, Secrets, and Custom Resources pertaining to this deployment.

Prometheus Operator introduces a set of [Custom Resource Definitions](https://github.com/prometheus-operator/prometheus-operator#customresourcedefinitions) that allow users to deploy and manage Prometheus and Alertmanager instances by creating and modifying those custom resources on a cluster.

Prometheus Operator will automatically update your Prometheus configuration based on the live state of these custom resources.

There are also certain special types of ConfigMaps/Secrets such as those corresponding to Grafana Dashboards, Grafana Datasources, and Alertmanager Configs that will automatically update your Prometheus configuration via sidecar proxies that observe the live state of those resources within your cluster.

By default, a set of these resources is created within your cluster as part of deploying this chart to set up a basic Monitoring/Alerting stack. We use the ones generated by [kube-prometheus.](https://github.com/prometheus-operator/kube-prometheus) For more information how to configure custom targets, alerts, notifiers, and dashboards after deploying the chart, see below.

# Configuring Targets with ServiceMonitors and PodMonitors

Customizing the scrape configuration used by Prometheus to determine which resources to scrape metrics from will primarily involve creating / modifying the following resources within your cluster:

### ServiceMonitors

This CRD declaratively specifies how groups of Kubernetes services should be monitored. Any Services in your cluster that match the labels located within the ServiceMonitor selector field will be monitored based on the endpoints specified on the ServiceMonitor. For more information on what fields can be specified, please look at the [spec](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#servicemonitor) provided by Prometheus Operator.

For more information about how ServiceMonitors work, refer to the [Prometheus documentation.](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/running-exporters.md)

### PodMonitors

This CRD declaratively specifies how group of pods should be monitored. Any Pods in your cluster that match the labels located within the PodMonitor selector field will be monitored based on the podMetricsEndpoints specified on the PodMonitor. For more information on what fields can be specified, please look at the [spec](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#podmonitorspec) provided by Prometheus Operator.

# Configuring Alerts or Recording Rules with PrometheusRules

Customizing the alerts recorded by Prometheus and the configuration of notifiers that should notify users when alerts are triggered will primarily involve creating/modifying PrometheusRules and the Alertmanager config.

The Alertmanager needs to be a Secret that exists in the `cattle-monitoring` system namespace in order for that to be used.

### PrometheusRules

This CRD defines a desired set of Prometheus alerting and/or recording rules.

Creating the desired alerting rule will involve creating or updating a RuleGroup with your desired alerting rule. Each alerting rule needs to:

- Name the alert
- Provide a PromQL expression for the alert
- Provide any labels or annotations that need to be sent to Alertmanager on the alert

Labels are concise values that identify the alert (e.g. cluster name or severity), while Annotations contain other important pieces of information that need to be displayed when notifying users about an alert (e.g. summary, description, message, runbook URL, etc.).

For more information on what fields can be specified, please look at the [spec.](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#prometheusrulespec)

### Recording Rules

If you would like to add [recording rules](https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/) to define a frequently needed or computationally intensive PromQL query that should be stored as a new set of time series, you should create a PrometheusRule.

Creating the desired recording rule will involve creating or updating a RuleGroup with your desired recording rule.

Each recording rule needs to:

- Name the new time series
- Provide a PromQL expression for the series
- Provide any labels that need to be added to this series

For more information on what fields can be specified, please look at [this page.](https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/#rule)

### Alertmanager Config

The [Alertmanager Config](https://prometheus.io/docs/alerting/latest/configuration/#configuration-file) Secret contains the configuration of an Alertmanager instance that sends out notifications based on alerts it receives from Prometheus.

There only needs to be one Alertmanager in the cluster, and when `rancher-monitor` is installed, a default Alertmanager config is created.
 
This Secret should be updated or modified any time you want to:
 
- Add in new notifiers or receivers
- Change the alerts that should be sent to specific notifiers or receivers
- Change the group of alerts that are sent out

> Normally, the Alertmanager config secret is configured as a resource that's not managed by Helm. For the `rancher-monitoring` chart, the default case will leave that secret behind. You should save the Secret so that if you uninstall and reinstall `rancher-monitoring`, you wouldn't lose your alerting configuration.
 
For more information on what fields can be specified in this secret, please look at the [Prometheus Alertmanager docs](https://prometheus.io/docs/alerting/latest/alertmanager/)

The full spec for the Alertmanager configuration file and what it takes in can be found [here.](https://prometheus.io/docs/alerting/latest/configuration/#configuration-file)

It is recommended to use an Existing Secret within the namespace `cattle-monitoring-system` for the Alertmanager Config in order to avoid having to to manage the Secret via Helm.

Using the default Helm managed Secret will require a user to redeploy the chart in order to persist any changes to the Alertmanager configuration on a `helm upgrade`. Using an existing Secret will allow the secret to be directly modified to perform live updates on the Alertmanager configuration if necessary.

If using an Existing Secret, please add the label `alertmanager_config` to the Secret to enable modifying the secret using the Rancher Dashboard UI.

Each [Receiver](https://prometheus.io/docs/alerting/latest/configuration/#receiver) (an Alertmanager entity that describes the notification configuration of one or more providers) must define the notification configuration of exactly one provider (i.e. Slack, PagerDuty, OpsGenie, etc.).

In the Alertmanager documentation about routes, it is said that there must be one "configured top-level route, which must match all alerts (i.e. not have any configured matchers)." On this page, we refer to the notifier using that route as the master notifier. The other notifiers that are referenced in child routes (usually a list provided in `.routes`) we refer to as notifiers.

For more information, refer to the [official Prometheus documentation about configuring routes.](https://www.prometheus.io/docs/alerting/latest/configuration/#route)

# Trusted CA for Notifiers

If you need to add a trusted CA to your notifier, follow these steps:

1. Create the `cattle-monitoring-system` namespace.
1. Add your trusted CA secret to the `cattle-monitoring-system` namespace.
1. Deploy or upgrade the `rancher-monitoring` Helm chart. In the chart options, reference the secret in **Alerting > Additional Secrets.**

**Result:** The default Alertmanager custom resource will have access to your trusted CA.

# Additional Scrape Configurations

If the scrape configuration you want cannot be specified via a ServiceMonitor or PodMonitor at the moment, you can provide an additionalScrapeConfigSecret on deploying or upgrading `rancher-monitoring`.

A [scrape_config section](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config) specifies a set of targets and parameters describing how to scrape them. In the general case, one scrape configuration specifies a single job. 

If you want to enable Istio in the cluster, you might need to use an additional scrape configuration to ensure you can view traffic, metrics and graphs for resources deployed in other namespaces than only the `istio-system`.

To create an additional scrape configuration, refer to [this page.](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/additional-scrape-config.md)


# Examples

### ServiceMonitor

An example ServiceMonitor custom resource can be found [here.](https://github.com/prometheus-operator/prometheus-operator/blob/master/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml) 

### PodMonitor

An example PodMonitor can be found [here.](https://github.com/prometheus-operator/prometheus-operator/blob/master/example/user-guides/getting-started/example-app-pod-monitor.yaml) An example Prometheus resource that refers to it can be found [here.](https://github.com/prometheus-operator/prometheus-operator/blob/master/example/user-guides/getting-started/prometheus-pod-monitor.yaml)

### PrometheusRule

Prometheus rule files are held in PrometheusRule custom resources. Use the label selector field ruleSelector in the Prometheus object to define the rule files that you want to be mounted into Prometheus. An example PrometheusRule is on [this page.](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/alerting.md)

### Alertmanager Config

The following values should generate the Alertmanager Config YAML that will be stored under the key `alertmanager.yaml`. This is an example of a Config contained in this key that represents one master Slack notifier with one master Route and no other notifiers configured. 

```yaml
route:  
  group_by: ['job']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 3h 
  receiver: 'slack-notifications'
receivers:
- name: 'slack-notifications'
  slack_configs:
  - send_resolved: true
    text: '{{ template "slack.rancher.text" . }}'
    api_url: <user-provided slack webhook url here>
templates:
- /etc/alertmanager/config/*.tmpl
```

The following is the default Alertmanager config secret deployed by the `rancher-monitoring chart`. This example shows one master Custom Receiver with one Master Route, one other Route, and no other notifiers:

```yaml
global:
  resolve_timeout: 5m
route:
  group_by: ['job']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: 'null'
  routes:
  - group_by: ['job']
    group_wait: 30s
    group_interval: 5m
    repeat_interval: 12h
    match:
      alertname: Watchdog
    receiver: 'slack'
receivers:
- name: 'null'
- name: 'slack'
templates:
- /etc/alertmanager/config/*.tmpl
```